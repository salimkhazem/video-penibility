# ğŸ¬ Video Penibility Assessment Framework

A comprehensive deep learning framework for assessing physical penibility (strain/difficulty) in videos using multiple feature extraction methods and temporal modeling approaches.

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![TensorBoard](https://img.shields.io/badge/TensorBoard-Enabled-orange.svg)](https://www.tensorflow.org/tensorboard)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

## ğŸš€ **Key Features**

### **ğŸ§  Multiple Feature Extraction Methods**
- **I3D**: Inflated 3D ConvNets for video understanding
- **Swin3D**: 3D Swin Transformer variants (Tiny, Small, Base)
- **FaceNet**: Face-based feature extraction
- **FaceMesh**: 3D facial landmark detection
- **LibreFace**: Advanced facial analysis features

### **ğŸ¯ Advanced Temporal Modeling**
- **Transformer**: Multi-head attention for sequence modeling
- **TCN**: Temporal Convolutional Networks
- **LSTM/GRU**: Recurrent neural networks for temporal dependencies

### **ğŸ“Š Comprehensive Monitoring & Visualization**
- **TensorBoard Integration**: Real-time training monitoring
- **Prediction vs Target Plots**: Visual assessment of model performance
- **Rich Console Output**: Beautiful training progress with Rich library
- **Cross-Validation Metrics**: MSE, MAE, RÂ², CCC with statistical analysis

### **âš™ï¸ Robust Training Framework**
- **Subject-wise Cross-Validation**: Proper evaluation avoiding data leakage
- **Configurable YAML**: Easy experiment configuration and reproduction
- **Early Stopping**: Automatic training termination to prevent overfitting
- **Model Checkpointing**: Save best models during training
- **Gradient Clipping**: Stable training for sequence models

## ğŸ“ **Project Structure**

```
video_penibility_assessment/
â”œâ”€â”€ ğŸ“ src/video_penibility/           # Core framework
â”‚   â”œâ”€â”€ ğŸ“ config/                     # Configuration management
â”‚   â”œâ”€â”€ ğŸ“ datasets/                   # Dataset loading and processing
â”‚   â”œâ”€â”€ ğŸ“ models/                     # Model architectures
â”‚   â”œâ”€â”€ ğŸ“ training/                   # Training framework
â”‚   â””â”€â”€ ğŸ“ utils/                      # Utility functions
â”œâ”€â”€ ğŸ“ scripts/                        # Training and evaluation scripts
â”œâ”€â”€ ğŸ“ configs/                        # Experiment configurations
â”œâ”€â”€ ğŸ“ results/                        # Training outputs and logs
â”œâ”€â”€ ğŸ“„ requirements.txt                # Python dependencies
â””â”€â”€ ğŸ“„ README.md                       # This file
```

## ğŸ› ï¸ **Installation**

### **Prerequisites**
- Python 3.8+
- CUDA-compatible GPU (recommended)
- 16GB+ RAM for large feature sets

### **Setup**

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd video_penibility_assessment
   ```

2. **Create virtual environment**
   ```bash
   python -m venv ai_vision
   source ai_vision/bin/activate  # Linux/Mac
   # or
   ai_vision\Scripts\activate     # Windows
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```

## ğŸ¯ **Quick Start**

### **1. Basic Training Example**

```bash
# Train I3D + TCN model
python scripts/train.py --config configs/i3d_tcn.yaml

# Train Swin3D + Transformer model
python scripts/train.py --config configs/swin3d_transformer_custom.yaml
```

### **2. Monitor Training with TensorBoard**

```bash
# Start TensorBoard (in separate terminal)
tensorboard --logdir=results/

# View at: http://localhost:6006
```

### **3. Create Custom Configuration**

```yaml
# configs/my_experiment.yaml
experiment:
  name: "my_experiment"
  device: "cuda:0"
  output_dir: "results/my_experiment"

data:
  features_type: "i3d"                    # i3d, swin3d_t, facenet, etc.
  data_type: "full_body"
  annotation_path: "/path/to/annotations.csv"
  features_root: "/path/to/features/"

model:
  name: "transformer"                     # transformer, tcn, lstm, gru
  hidden_dim: 512
  num_layers: 4
  dropout: 0.2

training:
  batch_size: 16
  learning_rate: 0.001
  num_epochs: 150
  optimizer: "adamw"
```

## ğŸ“Š **Experiment Results**

### **Current Best Performance**

| **Features** | **Model** | **MSE** | **RÂ²** | **CCC** | **Status** |
|-------------|-----------|---------|--------|---------|------------|
| I3D         | GRU       | 6.87Â±4.71 | 0.20   | **0.59** | âœ… Best |
| I3D         | TCN       | ~3.8    | ~0.5   | ~0.7    | ğŸ”„ Current |
| Swin3D-T    | Transformer | 10.07Â±1.25 | -0.22 | -0.004 | âŒ Poor |

### **Features Comparison**
- **I3D (1024-dim)**: Better for motion understanding
- **Swin3D-T (768-dim)**: More compact but lower performance
- **Cross-validation**: 5-fold subject-wise splitting

## ğŸ”§ **Configuration Options**

### **Supported Features**
```yaml
features_type:
  - "i3d"              # Inflated 3D ConvNets
  - "swin3d_t"         # Swin3D Tiny
  - "swin3d_s"         # Swin3D Small  
  - "swin3d_b"         # Swin3D Base
  - "facenet"          # Face features
  - "facemesh"         # Facial landmarks
  - "libreface"        # Advanced face analysis
```

### **Supported Models**
```yaml
model_name:
  - "transformer"      # Multi-head attention
  - "tcn"             # Temporal Convolutional Network
  - "lstm"            # Long Short-Term Memory
  - "gru"             # Gated Recurrent Unit
```

### **Training Options**
```yaml
training:
  optimizer: ["adam", "adamw", "sgd"]
  loss_function: ["mse", "mae", "huber"]
  scheduler: ["step", "plateau", null]
  early_stopping_patience: 30
```

## ğŸ“ˆ **TensorBoard Visualizations**

The framework provides comprehensive logging:

- **ğŸ“Š Training/Validation Metrics**: Loss, MSE, MAE, RÂ², CCC
- **ğŸ¯ Prediction Plots**: Scatter plots with residuals analysis
- **ğŸ“ˆ Learning Curves**: Training progress over epochs
- **ğŸ” Model Architecture**: Network graph visualization
- **ğŸ“Š Cross-Validation Summary**: Statistical analysis across folds

## ğŸ§ª **Experiment Management**

### **Configuration-Based Experiments**
- All experiments defined in YAML files
- Easy parameter sweeps and ablation studies
- Automatic result organization by experiment name

### **Reproducibility**
- Fixed random seeds across experiments
- Version control for configurations
- Complete logging of hyperparameters

### **Cross-Validation**
- Subject-wise splitting (no data leakage)
- Statistical significance testing
- Variance analysis across folds

## ğŸ¤ **Contributing**

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

## ğŸ“„ **License**

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ **Acknowledgments**

- PyTorch team for the excellent deep learning framework
- TensorBoard for comprehensive experiment visualization
- Rich library for beautiful console output
- Contributors to the various feature extraction methods

## ğŸ“š **Citation**

If you use this framework in your research, please cite:

```bibtex
@software{video_penibility_framework,
  title={Video Penibility Assessment Framework},
  author={Your Name},
  year={2024},
  url={https://github.com/yourusername/video_penibility_assessment}
}
```

---

**ğŸ¯ Ready to assess video penibility with state-of-the-art deep learning!**
